name: Convert HuggingFace Model to GGUF
description: Downloads a pretrained model from HuggingFace Hub and converts it to GGUF format using llama.cpp converter. Supports various quantization levels for optimized inference.

inputs:
  - {name: model_id, type: String, description: 'HuggingFace model ID (e.g., meta-llama/Llama-2-7b-hf, mistralai/Mistral-7B-v0.1)'}
  - {name: hf_token, type: String, description: 'HuggingFace access token (optional, required for gated models)', default: ''}
  - {name: quantization_type, type: String, description: 'GGUF quantization type: f32, f16, q8_0, q5_1, q5_0, q4_1, q4_0, q2_K'}
  - {name: model_name_suffix, type: String, description: 'Suffix for output GGUF filename (e.g., my-model will create my-model.gguf)'}

outputs:
  - {name: gguf_model, type: Model}

metadata:
  annotations:
    author: Convert HuggingFace Model to GGUF Component

implementation:
  container:
    image: python:3.10
    command:
      - sh
      - -c
      - |
        set -e
        echo "Installing system dependencies..."
        apt-get update > /dev/null 2>&1
        apt-get install -y git build-essential > /dev/null 2>&1
        
        echo "Installing Python dependencies..."
        pip install --no-cache-dir huggingface_hub numpy sentencepiece protobuf > /dev/null 2>&1
        
        "$0" "$@"
      - python3
      - -u
      - -c
      - |
        def _make_parent_dirs_and_return_path(file_path):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path
        
        def convert_to_gguf(
            model_id,
            hf_token,
            quantization_type,
            model_name_suffix,
            gguf_model_path,
        ):
            import os
            import subprocess
            import logging
            import shutil
            from huggingface_hub import snapshot_download, login
            
            logging.basicConfig(level=logging.INFO)
            logger = logging.getLogger("gguf_converter")
            
            logger.info("=" * 70)
            logger.info("Starting HuggingFace to GGUF Conversion")
            logger.info("=" * 70)
            logger.info(f"Model ID: {model_id}")
            logger.info(f"Quantization type: {quantization_type}")
            logger.info(f"Output model name: {model_name_suffix}")
            logger.info(f"Output directory: {gguf_model_path}")
            
            # Validate quantization type
            valid_qtypes = ['f32', 'f16', 'q8_0', 'q5_1', 'q5_0', 'q4_1', 'q4_0', 'q2_K']
            if quantization_type not in valid_qtypes:
                raise ValueError(f"Invalid quantization type. Must be one of: {valid_qtypes}")
            
            # Handle HuggingFace token
            def get_token_value(token_arg):
                if not token_arg or token_arg.strip() == '':
                    return None
                token_str = str(token_arg).strip()
                if os.path.exists(token_str):
                    try:
                        with open(token_str) as f:
                            content = f.read().strip()
                            return content if content else None
                    except Exception as e:
                        logger.warning(f"Failed to read token file: {e}")
                        return None
                else:
                    return token_str if token_str and token_str != "None" else None
            
            token = get_token_value(hf_token)
            if token:
                logger.info("Logging in to HuggingFace with provided token...")
                try:
                    login(token=token)
                    logger.info("Successfully logged in to HuggingFace")
                except Exception as e:
                    logger.warning(f"Failed to login with token: {e}")
            else:
                logger.info("No HuggingFace token provided, proceeding with public access")
            
            # Download model from HuggingFace
            logger.info("=" * 70)
            logger.info(f"Downloading model from HuggingFace: {model_id}")
            logger.info("This may take several minutes depending on model size...")
            logger.info("=" * 70)
            
            model_download_dir = "/tmp/hf_model"
            if os.path.exists(model_download_dir):
                shutil.rmtree(model_download_dir)
            
            try:
                downloaded_model_path = snapshot_download(
                    repo_id=model_id,
                    cache_dir=model_download_dir,
                    token=token,
                    ignore_patterns=["*.md", "*.txt", ".gitattributes"]
                )
                logger.info(f"Model downloaded successfully to: {downloaded_model_path}")
            except Exception as e:
                logger.error(f"Failed to download model: {e}")
                raise RuntimeError(f"Model download failed: {e}")
            
            # Create output directory
            os.makedirs(gguf_model_path, exist_ok=True)
            
            # Clone llama.cpp repository
            logger.info("Cloning llama.cpp repository...")
            llama_cpp_dir = "/tmp/llama.cpp"
            if os.path.exists(llama_cpp_dir):
                shutil.rmtree(llama_cpp_dir)
            
            clone_cmd = ["git", "clone", "https://github.com/ggml-org/llama.cpp.git", llama_cpp_dir]
            result = subprocess.run(clone_cmd, capture_output=True, text=True)
            if result.returncode != 0:
                logger.error(f"Failed to clone llama.cpp: {result.stderr}")
                raise RuntimeError("Failed to clone llama.cpp repository")
            logger.info("llama.cpp cloned successfully")
            
            # Install llama.cpp Python requirements
            logger.info("Installing llama.cpp conversion requirements...")
            requirements_file = os.path.join(llama_cpp_dir, "requirements", "requirements-convert_hf_to_gguf.txt")
            if os.path.exists(requirements_file):
                pip_cmd = ["pip", "install", "-r", requirements_file]
                subprocess.run(pip_cmd, capture_output=True, check=True)
                logger.info("Requirements installed successfully")
            else:
                logger.warning("requirements-convert_hf_to_gguf.txt not found, using base requirements")
                pip_cmd = ["pip", "install", "gguf", "numpy", "sentencepiece", "protobuf"]
                subprocess.run(pip_cmd, capture_output=True, check=True)
            
            # Path to conversion script
            convert_script = os.path.join(llama_cpp_dir, "convert_hf_to_gguf.py")
            if not os.path.exists(convert_script):
                # Fallback to older script name
                convert_script = os.path.join(llama_cpp_dir, "convert.py")
            
            if not os.path.exists(convert_script):
                raise FileNotFoundError(f"Conversion script not found in {llama_cpp_dir}")
            
            logger.info(f"Using conversion script: {convert_script}")
            
            # Define output file path
            output_gguf_file = os.path.join(gguf_model_path, f"{model_name_suffix}.gguf")
            
            # Convert model to GGUF
            logger.info("=" * 70)
            logger.info("Converting model to GGUF format...")
            logger.info(f"This may take several minutes depending on model size...")
            logger.info("=" * 70)
            
            convert_cmd = [
                "python3",
                convert_script,
                downloaded_model_path,
                "--outfile", output_gguf_file,
                "--outtype", quantization_type
            ]
            
            logger.info(f"Running conversion command: {' '.join(convert_cmd)}")
            
            try:
                result = subprocess.run(
                    convert_cmd,
                    capture_output=True,
                    text=True,
                    check=True
                )
                logger.info("Conversion output:")
                logger.info(result.stdout)
                if result.stderr:
                    logger.warning(f"Conversion warnings: {result.stderr}")
            except subprocess.CalledProcessError as e:
                logger.error(f"Conversion failed with error code {e.returncode}")
                logger.error(f"STDOUT: {e.stdout}")
                logger.error(f"STDERR: {e.stderr}")
                raise RuntimeError(f"Model conversion failed: {e.stderr}")
            
            # Verify output file exists
            if not os.path.exists(output_gguf_file):
                raise FileNotFoundError(f"GGUF file was not created at {output_gguf_file}")
            
            # Get file size
            file_size_mb = os.path.getsize(output_gguf_file) / (1024 * 1024)
            
            # Create metadata file
            metadata = {
                "model_name": model_name_suffix,
                "source_model_id": model_id,
                "quantization_type": quantization_type,
                "gguf_file": f"{model_name_suffix}.gguf",
                "file_size_mb": round(file_size_mb, 2),
                "conversion_tool": "llama.cpp",
            }
            
            import json
            metadata_file = os.path.join(gguf_model_path, "conversion_metadata.json")
            with open(metadata_file, "w") as f:
                json.dump(metadata, f, indent=2)
            
            logger.info("=" * 70)
            logger.info("Conversion Complete!")
            logger.info("=" * 70)
            logger.info(f"Source model: {model_id}")
            logger.info(f"GGUF model saved to: {output_gguf_file}")
            logger.info(f"File size: {file_size_mb:.2f} MB")
            logger.info(f"Quantization: {quantization_type}")
            logger.info(f"Metadata saved to: {metadata_file}")
            logger.info("=" * 70)
            logger.info("Ready for upload to HuggingFace or local deployment!")
            logger.info("=" * 70)
        
        import argparse
        _parser = argparse.ArgumentParser(prog="Convert HuggingFace Model to GGUF", description="Downloads and converts a HuggingFace model to GGUF format")
        _parser.add_argument("--model_id", dest="model_id", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--hf_token", dest="hf_token", type=str, required=False, default='')
        _parser.add_argument("--quantization_type", dest="quantization_type", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model_name_suffix", dest="model_name_suffix", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--gguf_model", dest="gguf_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())
        convert_to_gguf(**_parsed_args)

    args:
      - --model_id
      - {inputValue: model_id}
      - --hf_token
      - {inputValue: hf_token}
      - --quantization_type
      - {inputValue: quantization_type}
      - --model_name_suffix
      - {inputValue: model_name_suffix}
      - --gguf_model
      - {outputPath: gguf_model}
