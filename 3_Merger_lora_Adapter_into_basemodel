name: Merge LoRA Adapter into Base Model
description: Merges fine-tuned LoRA adapters with a base model into a single deployable checkpoint. Supports Hugging Face Hub and local directories.

inputs:
  - {name: base_model, type: String, description: 'Base model path or Hugging Face ID (e.g., sshleifer/tiny-gpt2)'}
  - {name: adapter_dir, type: Model, description: 'Path to fine-tuned LoRA adapter directory from Generic LoRA Fine-tuning component'}
  - {name: precision, type: String, description: 'Training precision: fp32, bf16, or fp16'}
  - {name: quantization_bits, type: Integer, description: 'Quantization bits (0, 4, or 8). 0 means no quantization'}

outputs:
  - {name: merged_model_dir, type: Model}

metadata:
  annotations:
    author: Merge LoRA Adapter Component

implementation:
  container:
    image: python:3.10
    command:
      - sh
      - -c
      - (pip install --no-cache-dir peft > /dev/null 2>&1) && "$0" "$@"
      - python3
      - -u
      - -c
      - |
        def _make_parent_dirs_and_return_path(file_path):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path
        
        def merge_lora_adapter(
            base_model,
            adapter_dir,
            precision,
            quantization_bits,
            output_dir,
        ):
            import os
            import torch
            from peft import PeftModel
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import logging
            
            logging.basicConfig(level=logging.INFO)
            logger = logging.getLogger("merge_adapter")
            
            logger.info("=" * 60)
            logger.info("Starting LoRA Adapter Merge Process")
            logger.info("=" * 60)
            logger.info(f"Base model: {base_model}")
            logger.info(f"Input adapter directory: {adapter_dir}")
            
            # Fix: The actual adapter weights are in the 'adapter' subdirectory
            actual_adapter_path = os.path.join(adapter_dir, "adapter")
            
            # Verify the adapter directory exists
            if not os.path.exists(actual_adapter_path):
                logger.error(f"Adapter directory not found at: {actual_adapter_path}")
                logger.info(f"Contents of {adapter_dir}:")
                if os.path.exists(adapter_dir):
                    for item in os.listdir(adapter_dir):
                        logger.info(f"  - {item}")
                raise FileNotFoundError(f"Adapter directory not found at: {actual_adapter_path}")
            
            logger.info(f"Using adapter from: {actual_adapter_path}")
            logger.info(f"Output directory: {output_dir}")
            logger.info(f"Precision: {precision}")
            logger.info(f"Quantization bits: {quantization_bits}")
            
            os.makedirs(output_dir, exist_ok=True)
            
            logger.info("Loading tokenizer from base model...")
            tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                logger.info("Set pad_token to eos_token")
            logger.info(f"Tokenizer loaded successfully")
            
            logger.info("Loading base model from Hugging Face Hub...")
            
            model_kwargs = {"trust_remote_code": True}
            
            if precision == "bf16":
                model_kwargs["torch_dtype"] = torch.bfloat16
            elif precision == "fp16":
                model_kwargs["torch_dtype"] = torch.float16
            else:
                model_kwargs["torch_dtype"] = torch.float32
            
            try:
                model_kwargs["use_safetensors"] = True
                logger.info(f"Attempting to load with safetensors")
                base_model_obj = AutoModelForCausalLM.from_pretrained(base_model, **model_kwargs)
            except Exception as e:
                logger.warning(f"Failed to load with safetensors, trying without: {e}")
                model_kwargs.pop("use_safetensors", None)
                base_model_obj = AutoModelForCausalLM.from_pretrained(base_model, **model_kwargs)
            
            logger.info(f"Base model loaded successfully: {type(base_model_obj).__name__}")
            
            # Use the actual adapter path (inside the 'adapter' subdirectory)
            logger.info(f"Loading LoRA adapter from: {actual_adapter_path}")
            peft_model = PeftModel.from_pretrained(base_model_obj, actual_adapter_path)
            logger.info("LoRA adapter loaded successfully")
            
            logger.info("Merging LoRA adapter weights into base model...")
            merged_model = peft_model.merge_and_unload()
            logger.info("Merge completed successfully")
            
            logger.info(f"Saving merged model to: {output_dir}")
            merged_model.save_pretrained(output_dir, safe_serialization=True)
            logger.info("Merged model saved")
            
            logger.info("Saving tokenizer...")
            tokenizer.save_pretrained(output_dir)
            logger.info("Tokenizer saved")
            
            logger.info("=" * 60)
            logger.info("Merge Complete!")
            logger.info(f"Merged model saved to: {output_dir}")
            logger.info("Ready for deployment or evaluation")
            logger.info("=" * 60)
        
        import argparse
        _parser = argparse.ArgumentParser(prog="Merge LoRA Adapter into Base Model", description="Merges fine-tuned LoRA adapters with a base model into a single deployable checkpoint")
        _parser.add_argument("--base_model", dest="base_model", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--adapter_dir", dest="adapter_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--precision", dest="precision", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--quantization_bits", dest="quantization_bits", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output_dir", dest="output_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())
        merge_lora_adapter(**_parsed_args)

    args:
      - --base_model
      - {inputValue: base_model}
      - --adapter_dir
      - {inputPath: adapter_dir}
      - --precision
      - {inputValue: precision}
      - --quantization_bits
      - {inputValue: quantization_bits}
      - --output_dir
      - {outputPath: merged_model_dir}


